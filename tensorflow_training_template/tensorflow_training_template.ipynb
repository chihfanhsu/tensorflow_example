{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Editted by Silver on 2019/03/06\n",
    "\n",
    "This templateis for a tensorflow starter who needs a template to train the network.\n",
    "\n",
    "This file contains three parts\n",
    "\n",
    "1. prpare inputs\n",
    "2. Model Setup\n",
    "3. Training\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pickle\n",
    "import tqdm\n",
    "import config\n",
    "conf, _ = config.get_config()\n",
    "\n",
    "# Loading target model from a given file name\n",
    "if conf.tar_model == '[filename of your target model]':\n",
    "    import '''filename of your model''' as model\n",
    "    print('Load', conf.tar_model)\n",
    "else:\n",
    "    sys.exit(\"Sorry, Wrong Model!\")\n",
    "\n",
    "# Setting visable GPU\n",
    "# Here I set CUDA to only see one GPU\n",
    "gpus = [conf.gpu]\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=','.join([str(i) for i in gpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting the dataset in the 'dataset' folder\n",
    "data_dir = './dataset/'\n",
    "\n",
    "# Setting a folder for certain model\n",
    "model_dir = './'+conf.tar_model+'/'\n",
    "\n",
    "# Creating the 'logs' folder with respect to model folder to save the trained model\n",
    "logs_dir = model_dir + 'logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# 2. Prepare inputs #\n",
    "#####################\n",
    "\n",
    "# load data from pickle, a file structure for python\n",
    "with open(data_dir+'/[filename of your dataset].pickle', 'rb') as f:\n",
    "    raw_data = pickle.load(f)\n",
    "\n",
    "# Having a glance at raw_data\n",
    "print(type(raw_data))\n",
    "print(raw_data.shape)\n",
    "\n",
    "# You can preprocess the raw data here\n",
    "processed_data = raw_data\n",
    "\n",
    "# Having a glance at processed data\n",
    "print(type(processed_data))\n",
    "print(processed_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting a batch generator for getting a batch of dataset\n",
    "def batch_generator(data, idx, batch_size = 128):\n",
    "    np.random.shuffle(idx)\n",
    "    while(True):\n",
    "        for i in range(int(len(idx)/batch_size)):\n",
    "            yield data[idx[i*batch_size:((i+1)*batch_size)]]\n",
    "            # You might have the training labels, and the output will be \n",
    "            # yield data[idx[i*batch_size:((i+1)*batch_size)]], yield label[idx[i*batch_size:((i+1)*batch_size)]]\n",
    "            \n",
    "# splitting training and validation sets\n",
    "def get_generators(training_data, tv_ratio = 0.8, shuffle = True, batch_size = 128):\n",
    "    \n",
    "    print('Size of all training data', training_data.shape)\n",
    "    idx = np.arange(0, training_data.shape[0], 1)\n",
    "    if shuffle:\n",
    "        print('*Shuffle before training and validation split')\n",
    "        np.random.shuffle(idx)\n",
    "    training_idx = idx[:(int(tv_ratio*len(idx)))]\n",
    "    validation_idx =idx[int(tv_ratio*len(idx)):]\n",
    "    if (len(training_idx) + len(validation_idx) - len(idx)) > 0:\n",
    "        print('Some data are duplicated in the training and validation split')\n",
    "    elif (len(training_idx) + len(validation_idx) - len(idx)) < 0:\n",
    "        print('Some data are missing in the training and validation split')\n",
    "\n",
    "    training_gen = batch_generator(training_data, training_idx, batch_size)\n",
    "    validation_gen = batch_generator(training_data, validation_idx, batch_size)\n",
    "    return training_gen, len(training_idx), validation_gen, len(validation_idx)\n",
    "\n",
    "# defining the generators for training/validation/testing\n",
    "if conf.training:\n",
    "    get_training_batch, n_training_samples, get_validation_batch, n_validation_samples = get_generators(processed_data, tv_ratio = 0.8, shuffle = True, batch_size = conf.batch_size) # (128, 30, 450)\n",
    "else:\n",
    "    testing_idx = np.arange(0, processed_data.shape[0], 1)\n",
    "    get_testing_batch = batch_generator(processed_data, testing_idx, conf.batch_size)\n",
    "    \n",
    "# Havnig a glance at the output of batch_generator\n",
    "# this example is for autoencoder (only containing one output)\n",
    "# training_batch = next(get_training_batch)\n",
    "# print(training_batch.shape)\n",
    "\n",
    "# Usually having two output, a training batch and the corresponding label batch\n",
    "# training_batch, label_batch= next(get_training_batch)\n",
    "# print(training_batch.shape)\n",
    "# print(label_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearing tensorflow graph\n",
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default() as g:\n",
    "    \n",
    "    ##################\n",
    "    # 2. Model Setup #\n",
    "    ##################\n",
    "    \n",
    "    # Model input/label\n",
    "    model_input = tf.placeholder(tf.float32, shape=[conf.batch_size, '''Shape of the feature for trainging'''], name=\"model_input\")\n",
    "    model_GT = tf.placeholder(tf.float32, shape=[conf.batch_size, '''Shape of the feature for trainging'''], name='model_GT')\n",
    "    # Ratio for the dropout (never used in this example)\n",
    "    # keep_probability = tf.placeholder(tf.float32, name=\"keep_probabilty\")\n",
    "    # Tag for indicating training phase\n",
    "    train_phase = tf.placeholder(tf.bool, name='phase_train')\n",
    "\n",
    "    # Build the model\n",
    "    code, model_out = model.inference(model_input, keep_probability, train_phase)\n",
    "\n",
    "    # Check # weights contained in the model\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        shape = variable.get_shape()\n",
    "        variable_parameters = 1\n",
    "        for dim in shape:\n",
    "            variable_parameters *= dim.value\n",
    "\n",
    "        total_parameters += variable_parameters\n",
    "    print('Total parameters', total_parameters)\n",
    "    \n",
    "    # Loss functions (MSE here)\n",
    "    loss = tf.reduce_mean(tf.losses.mean_squared_error(predictions=model_out, labels=model_GT))\n",
    "    # save to summary for monitor\n",
    "    loss_summary = tf.summary.scalar(\"Tot_loss\", loss)\n",
    "    \n",
    "    # Defining the optimizer and update algorithm (BP)\n",
    "    def train(loss_val, var_list):\n",
    "        optimizer = tf.train.AdamOptimizer(conf.lr)\n",
    "        grads = optimizer.compute_gradients(loss_val, var_list=var_list)\n",
    "        return optimizer.apply_gradients(grads)\n",
    "\n",
    "    trainable_var = tf.trainable_variables()\n",
    "    \n",
    "    train_op = train(loss, trainable_var)\n",
    "    \n",
    "    # This op is used for updating the parameters in the batch normalization layers\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    # Preparing training saver for save the model and summary\n",
    "    # and defining a session for training\n",
    "    \n",
    "    print(\"Setting up summary op...\")\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "    print(\"Setting up Saver...\")\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # Defining the session\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False), graph=g)\n",
    "    \n",
    "    # if testing, load pretrained model from the 'logs' folder\n",
    "    if (conf.training == False):\n",
    "        ckpt = tf.train.get_checkpoint_state(logs_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            print('Loading sucessfully')\n",
    "        else:\n",
    "            print('No checkpoint file found')\n",
    "            raise\n",
    "    # otherwise initial all model weights\n",
    "    else:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "    # Creating two summary writers to show training loss and validation loss in the same graph\n",
    "    train_writer = tf.summary.FileWriter(logs_dir+'/train', sess.graph)\n",
    "    validation_writer = tf.summary.FileWriter(logs_dir+'/validation', sess.graph)\n",
    "    \n",
    "    ###############\n",
    "    # 3. Training #\n",
    "    ###############\n",
    "    if conf.training == True:\n",
    "        print(\"-------- Start Training --------\")\n",
    "        max_validloss = 99999\n",
    "        \n",
    "        for itr in range(int(conf.MAX_ITERATION)):\n",
    "            # Preparing training input\n",
    "            batch_xs = next(get_training_batch)\n",
    "            \n",
    "            # Training\n",
    "            sess.run([train_op,extra_update_ops], feed_dict={model_input: batch_xs,\n",
    "                                                             model_GT: batch_xs,\n",
    "                                                             # keep_probability: 0.4,\n",
    "                                                             train_phase:conf.training})\n",
    "            # have a glance to the training loss for a batch\n",
    "            if itr % 500 == 0:\n",
    "                train_loss, summary_str = sess.run([loss, loss_summary], feed_dict={model_input: batch_xs,\n",
    "                                                                                    model_GT: batch_xs,\n",
    "                                                                                    # keep_probability: 1,\n",
    "                                                                                    train_phase:False})\n",
    "                print(\"[T] Step: %d, loss:%g\" % (itr, np.mean(train_loss)))\n",
    "                train_writer.add_summary(summary_str, itr)\n",
    "                \n",
    "            # Validation\n",
    "            if itr % 1000 == 0:\n",
    "                # prepare inputs\n",
    "                valid_losses = []\n",
    "                for i in tqdm.trange(int(n_validation_samples/conf.batch_size)):\n",
    "                    batch_xs_valid = next(get_validation_batch)\n",
    "                    \n",
    "                    valid_loss, summary_sva=sess.run([loss, loss_summary], feed_dict={model_input: batch_xs_valid,\n",
    "                                                                                     model_GT: batch_xs_valid,\n",
    "                                                                                     # keep_probability: 1,\n",
    "                                                                                     train_phase:False})\n",
    "                    valid_losses.append(valid_loss)\n",
    "                    \n",
    "                # Saving validation log\n",
    "                validation_writer.add_summary(summary_sva, itr)\n",
    "                # Saving the ckpt if reaching better loss\n",
    "                calc_v_loss = np.mean(valid_losses)\n",
    "\n",
    "                if calc_v_loss < max_validloss:\n",
    "                    saver.save(sess, logs_dir + \"model.ckpt\", itr)\n",
    "                    print(\"[V*] Step: %d, loss:%g\" % (itr, calc_v_loss))\n",
    "                    max_validloss = calc_v_loss\n",
    "                else:\n",
    "                    print(\"[V] Step: %d, loss:%g\" % (itr, calc_v_loss))\n",
    "    else:\n",
    "        print(\"Start Testing....\")\n",
    "\n",
    "        test_losses = []\n",
    "        for i in tqdm.trange(int(processed_data.shape[0]/conf.batch_size)):\n",
    "            batch_xs_test = next(get_testing_batch)\n",
    "\n",
    "            test_loss, summary_sva=sess.run([loss, loss_summary], feed_dict={model_input: batch_xs_test,\n",
    "                                                                             model_GT: batch_xs_test,\n",
    "                                                                             # keep_probability: 1,\n",
    "                                                                             train_phase:False})\n",
    "            test_losses.append(test_loss)\n",
    "            \n",
    "            ''' You might want save the results, and you can add here'''\n",
    "\n",
    "\n",
    "        # See the average loss here\n",
    "        calc_test_loss = np.mean(test_losses)\n",
    "        print(\"[Test] Avg. loss:%g\" % (calc_test_loss))\n",
    "        \n",
    "    # finish\n",
    "    sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
