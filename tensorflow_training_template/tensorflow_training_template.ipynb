{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mmnet\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(MAX_ITERATION=10000.0, batch_size=128, debug=False, gpu=0, lr=0.0001, tar_model='model_1', training=True)\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import mmread\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import config\n",
    "conf, _ = config.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put yor dataset in the dataset folder\n",
    "data_dir = './dataset/'\n",
    "\n",
    "# setting a folder for certain model\n",
    "model_dir = './'+conf.tar_model+'/'\n",
    "\n",
    "# creating the logs folder in the respective model folder\n",
    "logs_dir = model_dir + 'logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model_1\n"
     ]
    }
   ],
   "source": [
    "# loading the target model as model\n",
    "if conf.tar_model == '[filename of your model]':\n",
    "    import '''filename of your model''' as model\n",
    "    print('Load', conf.tar_model)\n",
    "else:\n",
    "    sys.exit(\"Sorry, Wrong Model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir+'/[filename of your dataset].pickle', 'rb') as f:\n",
    "    raw_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(195000, 120, 450)\n"
     ]
    }
   ],
   "source": [
    "# Have a glance at raw_data\n",
    "print(type(raw_data))\n",
    "print(raw_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can add a preprocessing for the data\n",
    "traing_data = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(780000, 13500)\n"
     ]
    }
   ],
   "source": [
    "# Have a glance at traing_data\n",
    "print(type(traing_data))\n",
    "print(traing_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# setting a batch generator for getting a batch of dataset\n",
    "def batch_generator(traing_data = traing_data, batch_size = 128):\n",
    "    print('Size of all training data', traing_data.shape)\n",
    "    # while (1):\n",
    "    idx = np.arange(0, traing_data.shape[0], 1)\n",
    "    np.random.shuffle(idx)\n",
    "    while(True):\n",
    "        for i in range(int(len(idx)/batch_size)):\n",
    "            yield traing_data[idx[i*batch_size:((i+1)*batch_size)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the batch_generator\n",
    "get_batch = batch_generator(traing_data, conf.batch_size) # (B, Shape of your dataset)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Have a glance at the output of batch_generator\n",
    "# b = next(get_batch)\n",
    "# print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_parameters 120339388\n",
      "Setting up summary op...\n",
      "Setting up Saver...\n",
      "Start Testing\n",
      "Size of all training data (780000, 13500)\n",
      "[T] Step: 0, loss:0.154082\n",
      "[T] Step: 500, loss:0.00308652\n",
      "[T] Step: 1000, loss:0.00382808\n",
      "[T] Step: 1500, loss:0.00503836\n"
     ]
    }
   ],
   "source": [
    "# clear tensorflow graph\n",
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default() as g:\n",
    "    \n",
    "    ###############\n",
    "    # Model Setup #\n",
    "    ###############\n",
    "    \n",
    "    # model input\n",
    "    model_input = tf.placeholder(tf.float32, shape=[conf.batch_size, '''Shape of the feature for trainging'''], name=\"model_input\")\n",
    "    model_GT = tf.placeholder(tf.float32, shape=[conf.batch_size, '''Shape of the feature for trainging'''], name='model_GT')\n",
    "    keep_probability = tf.placeholder(tf.float32, name=\"keep_probabilty\")\n",
    "    train_phase = tf.placeholder(tf.bool, name='phase_train')\n",
    "\n",
    "    # build model\n",
    "    code, model_out = model.inference(model_input, keep_probability, train_phase)\n",
    "\n",
    "    # check # parameter of the model\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        # shape is an array of tf.Dimension\n",
    "        shape = variable.get_shape()\n",
    "        #   print(shape)\n",
    "        #   print(len(shape))\n",
    "        variable_parameters = 1\n",
    "        for dim in shape:\n",
    "        #   print(dim)\n",
    "            variable_parameters *= dim.value\n",
    "        #   print(variable_parameters)\n",
    "        total_parameters += variable_parameters\n",
    "    print('Total parameters', total_parameters)\n",
    "    \n",
    "    # model loss\n",
    "    loss = tf.reduce_mean(tf.losses.mean_squared_error(predictions=model_out, labels=model_GT))\n",
    "    loss_summary = tf.summary.scalar(\"Tot_loss\", loss)\n",
    "\n",
    "    def train(loss_val, var_list):\n",
    "        optimizer = tf.train.AdamOptimizer(conf.lr)\n",
    "        grads = optimizer.compute_gradients(loss_val, var_list=var_list)\n",
    "        return optimizer.apply_gradients(grads)\n",
    "\n",
    "    trainable_var = tf.trainable_variables()\n",
    "    \n",
    "    train_op = train(loss, trainable_var)\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    # setup\n",
    "    print(\"Setting up summary op...\")\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "    print(\"Setting up Saver...\")\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False), graph=g)\n",
    "    \n",
    "    if (conf.training == False):\n",
    "        ckpt = tf.train.get_checkpoint_state(logs_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            print('Loading sucessfully')\n",
    "        else:\n",
    "            print('No checkpoint file found')\n",
    "            raise\n",
    "    else:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "    # create two summary writers to show training loss and validation loss in the same graph\n",
    "    # need to create two folders 'train' and 'validation' inside FLAGS.logs_dir\n",
    "    train_writer = tf.summary.FileWriter(logs_dir+'/train', sess.graph)\n",
    "    validation_writer = tf.summary.FileWriter(logs_dir+'/validation', sess.graph)\n",
    "    \n",
    "    #########\n",
    "    # Start #\n",
    "    #########\n",
    "    if conf.training == True:\n",
    "        print(\"-------- Start Training --------\")\n",
    "        max_validloss = 99999\n",
    "        for itr in range(int(conf.MAX_ITERATION)):\n",
    "            \n",
    "            # get training input from the generator\n",
    "            batch_xs = next(get_batch)\n",
    "            \n",
    "            # training\n",
    "            sess.run([train_op,extra_update_ops], feed_dict={model_input: batch_xs,\n",
    "                                                             model_GT: batch_xs,\n",
    "                                                             keep_probability: 0.85,\n",
    "                                                             train_phase:conf.training})\n",
    "            # print training loss per 500 updates\n",
    "            if itr % 500 == 0:\n",
    "                train_loss, summary_str = sess.run([loss, loss_summary], feed_dict={model_input: batch_xs,\n",
    "                                                                                     model_GT: batch_xs,\n",
    "                                                                                     keep_probability: 1,\n",
    "                                                                                     train_phase:conf.training})\n",
    "                print(\"[T] Step: %d, loss:%g\" % (itr, np.mean(train_loss)))\n",
    "                train_writer.add_summary(summary_str, itr)\n",
    "            \n",
    "            # if you have the validation dataset you can\n",
    "            # validate the trained model per 1000 updates\n",
    "#             if itr % 1000 == 0:\n",
    "#                 # prepare inputs\n",
    "#                 valid_losses = []\n",
    "                \n",
    "#                 # scan all the validation dataset\n",
    "#                 for i in tqdm.trange(int('''size of your validation dataset'''/conf.batch_size)):\n",
    "#                     batch_xs_valid, batch_ys_valid = next(valid_batches)\n",
    "                    \n",
    "#                     # getting a validation batch\n",
    "#                     batch_ymap_valid = pts2map(batch_ys_valid);# print(batch_ymap_valid.shape)\n",
    "\n",
    "#                     feed_dict = {image: batch_xs_valid,\n",
    "#                                  annotation: batch_ymap_valid,\n",
    "#                                  keep_probability: 1.0,\n",
    "#                                  train_phase:conf.training}\n",
    "#                     # feeding the validation model to the trained model\n",
    "#                     valid_loss, pts_maps, summary_sva=sess.run([loss, heat_map, loss_summary], feed_dict=feed_dict)\n",
    "#                     # save all the losses of validation batches\n",
    "#                     valid_losses.append(valid_loss)\n",
    "                    \n",
    "#                 # save validation log\n",
    "#                 validation_writer.add_summary(summary_sva, itr)\n",
    "                \n",
    "#                 # save the ckpt to logs folder if reachings better average validation loss\n",
    "#                 calc_v_loss = np.mean(valid_losses)\n",
    "#                 if calc_v_loss < max_validloss:\n",
    "#                     saver.save(sess, logs_dir + \"model.ckpt\", itr)\n",
    "#                     print(\"[V*] Step: %d, loss:%g\" % (itr, calc_v_loss))\n",
    "#                     max_validloss = calc_v_loss\n",
    "#                 else:\n",
    "#                     print(\"[V] Step: %d, loss:%g\" % (itr, calc_v_loss))\n",
    "                    \n",
    "    # testing model\n",
    "    else:\n",
    "        print(\"Testing\")\n",
    "#         testing_batch = 2\n",
    "#         inferred_map = []\n",
    "#         for t in tqdm.trange(int(db_helen['img']['testset'].shape[0]/testing_batch)):\n",
    "#             t_batch_x = db_helen['img']['testset'][(t*testing_batch):((t+1)*testing_batch)]\n",
    "#             t_batch_y = db_helen['pts']['testset'][(t*testing_batch):((t+1)*testing_batch)]\n",
    "#             feed_dict = {image: t_batch_x, keep_probability: 1.0,train_phase:conf.training}\n",
    "\n",
    "#             batch_map=sess.run(heat_map, feed_dict=feed_dict)\n",
    "#             inferred_map.append(batch_map)\n",
    "\n",
    "#         inferred_map = np.asarray(inferred_map)\n",
    "#         pts_maps = np.reshape(inferred_map, newshape=(-1,inferred_map.shape[2],inferred_map.shape[3],inferred_map.shape[4]))\n",
    "#         infered_pts = map2pts(pts_maps)\n",
    "\n",
    "#         for idx, content in enumerate(zip(db_helen['img']['testset'],infered_pts)):\n",
    "#             img = content[0].copy()\n",
    "#             img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "#             for kp_idx, keypoint in enumerate(content[1]):\n",
    "#                 cv2.circle(img,(keypoint[0],keypoint[1]), 2, (0,255,0), -1)\n",
    "#                 cv2.putText(img, str(kp_idx), (keypoint[0],keypoint[1]), cv2.FONT_HERSHEY_SIMPLEX, 0.3,(255,255,255),1,cv2.LINE_AA)\n",
    "\n",
    "#             cv2.imwrite(pred_dir + str(idx)+ '.png', img) \n",
    "\n",
    "#         norm_error_image = eval_norm_error_image(infered_pts, db_helen['pts']['testset'])\n",
    "#         pandas.DataFrame({'loss':norm_error_image}).to_csv(model_dir + 'norm_error_image_' + str(conf.testing)+ '.csv')\n",
    "    \n",
    "    # finish training\n",
    "    sess.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
