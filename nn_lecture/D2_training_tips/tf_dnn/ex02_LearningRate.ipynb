{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import [package name] as [name abbr.]\n",
    "# python 處理數字跟 tensor 運算的主要套件\n",
    "import numpy as np\n",
    "# google 的 NN coding 套件\n",
    "import tensorflow as tf\n",
    "\n",
    "# 有可能會出現警告，但是可以不用理他\n",
    "# c:\\users\\silver\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:36:\n",
    "# FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating`\n",
    "# is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
    "# from ._conv import register_converters as _register_converters\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exec(open(\"00_readingInput.py\").read())\n",
    "feature_dim = X_train.shape[1]\n",
    "classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model Input (x) and Output (y_),  y_ = f(x)\n",
    "x = tf.placeholder(tf.float32, [None, X_train.shape[1]])\n",
    "y_ = tf.placeholder(tf.int32, [None])\n",
    "y_one = tf.one_hot(y_,classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Define Your Model Here"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ex 02\n",
    "嘗試幾種不同的 leaning rate (0.1, 0.01, 0.001) 試試看訓練結果\n",
    "Hint:\n",
    "tf.train.GradientDescentOptimizer(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate 0.1\n",
    "lr1_h1 = tf.layers.dense(x,128,activation=None)\n",
    "lr1_h1_act = tf.nn.sigmoid(lr1_h1)\n",
    "lr1_h2 = tf.layers.dense(lr1_h1_act,256,activation=None)\n",
    "lr1_h2_act = tf.nn.sigmoid(lr1_h2)\n",
    "lr1_y = tf.layers.dense(lr1_h2_act,classes,activation=None)\n",
    "\n",
    "lr1_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_one, logits=lr1_y))\n",
    "lr1_train_step = tf.train.GradientDescentOptimizer( ).minimize(lr1_loss)\n",
    "\n",
    "# Accuracy of the Model\n",
    "lr1_y_pred = tf.argmax(tf.nn.softmax(lr1_y), 1, output_type=tf.int32)\n",
    "lr1_correct = tf.equal(lr1_y_pred, y_)\n",
    "lr1_accuracy = tf.reduce_mean(tf.cast(lr1_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate 0.01\n",
    "lr01_h1 = tf.layers.dense(x,128,activation=None)\n",
    "lr01_h1_act = tf.nn.sigmoid(lr01_h1)\n",
    "lr01_h2 = tf.layers.dense(lr01_h1_act,256,activation=None)\n",
    "lr01_h2_act = tf.nn.sigmoid(lr01_h2)\n",
    "lr01_y = tf.layers.dense(lr01_h2_act,classes,activation=None)\n",
    "\n",
    "lr01_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_one, logits=lr01_y))\n",
    "lr01_train_step = tf.train.GradientDescentOptimizer( ).minimize(lr01_loss)\n",
    "\n",
    "# Accuracy of the Model\n",
    "lr01_y_pred = tf.argmax(tf.nn.softmax(lr01_y), 1, output_type=tf.int32)\n",
    "lr01_correct = tf.equal(lr01_y_pred, y_)\n",
    "lr01_accuracy = tf.reduce_mean(tf.cast(lr01_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate 0.001\n",
    "lr001_h1 = tf.layers.dense(x,128,activation=None)\n",
    "lr001_h1_act = tf.nn.sigmoid(lr001_h1)\n",
    "lr001_h2 = tf.layers.dense(lr001_h1_act,256,activation=None)\n",
    "lr001_h2_act = tf.nn.sigmoid(lr001_h2)\n",
    "lr001_y = tf.layers.dense(lr001_h2_act,classes,activation=None)\n",
    "\n",
    "lr001_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_one, logits=lr001_y))\n",
    "lr001_train_step = tf.train.GradientDescentOptimizer( ).minimize(lr001_loss)\n",
    "\n",
    "# Accuracy of the Model\n",
    "lr001_y_pred = tf.argmax(tf.nn.softmax(lr001_y), 1, output_type=tf.int32)\n",
    "lr001_correct = tf.equal(lr001_y_pred, y_)\n",
    "lr001_accuracy = tf.reduce_mean(tf.cast(lr001_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the models\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 30\n",
    "batch_size = 16\n",
    "get_batches = gen_batches(X_train, Y_train, batch_size)\n",
    "batches_in_a_epoch = int(X_train.shape[0]/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train Model for 1000 steps\n",
    "lr1_hist_train_acc = []\n",
    "lr1_hist_valid_acc = []\n",
    "lr01_hist_train_acc = []\n",
    "lr01_hist_valid_acc = []\n",
    "lr001_hist_train_acc = []\n",
    "lr001_hist_valid_acc = []\n",
    "lr1_hist_train_loss = []\n",
    "lr1_hist_valid_loss = []\n",
    "lr01_hist_train_loss = []\n",
    "lr01_hist_valid_loss = []\n",
    "lr001_hist_train_loss = []\n",
    "lr001_hist_valid_loss = []\n",
    "for step in range(epoch*batches_in_a_epoch):\n",
    "    batch_xs, batch_ys = next(get_batches)\n",
    "    sess.run(lr1_train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    sess.run(lr01_train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    sess.run(lr001_train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    if (step % batches_in_a_epoch == 0):\n",
    "        # trainin and validation evaluation\n",
    "         # lr = 0.1\n",
    "        train_loss, train_acc = sess.run([lr1_loss,lr1_accuracy], feed_dict={x: X_train, y_: Y_train})\n",
    "        valid_loss, valid_acc = sess.run([lr1_loss,lr1_accuracy], feed_dict={x: X_test,  y_: Y_test})\n",
    "        lr1_hist_train_acc.append(train_acc)\n",
    "        lr1_hist_valid_acc.append(valid_acc)\n",
    "        lr1_hist_train_loss.append(train_loss)\n",
    "        lr1_hist_valid_loss.append(valid_loss)\n",
    "        #print(\"[lr=0.1] Accuracy: [T] %.4f / [V] %.4f\" % (train_acc,valid_acc))\n",
    "        # lr = 0.01\n",
    "        train_loss, train_acc = sess.run([lr01_loss,lr01_accuracy], feed_dict={x: X_train, y_: Y_train})\n",
    "        valid_loss, valid_acc = sess.run([lr01_loss,lr01_accuracy], feed_dict={x: X_test,  y_: Y_test})\n",
    "        lr01_hist_train_acc.append(train_acc)\n",
    "        lr01_hist_valid_acc.append(valid_acc)\n",
    "        lr01_hist_train_loss.append(train_loss)\n",
    "        lr01_hist_valid_loss.append(valid_loss)\n",
    "        #print(\"[lr=0.01] Accuracy: [T] %.4f / [V] %.4f\" % (train_acc,valid_acc))\n",
    "        # lr = 0.001\n",
    "        train_loss, train_acc = sess.run([lr001_loss,lr001_accuracy], feed_dict={x: X_train, y_: Y_train})\n",
    "        valid_loss, valid_acc = sess.run([lr001_loss,lr001_accuracy], feed_dict={x: X_test,  y_: Y_test})\n",
    "        lr001_hist_train_acc.append(train_acc)\n",
    "        lr001_hist_valid_acc.append(valid_acc)\n",
    "        lr001_hist_train_loss.append(train_loss)\n",
    "        lr001_hist_valid_loss.append(valid_loss)\n",
    "        #print(\"[lr=0.001] Accuracy: [T] %.4f / [V] %.4f\" % (train_acc,valid_acc))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0,figsize=(8,6))\n",
    "plt.subplot(121)\n",
    "x = [x for x in range(len(lr1_hist_train_loss))]\n",
    "lr1_line, = plt.plot(x, lr1_hist_train_loss, label='lr=0.1')\n",
    "lr01_line, = plt.plot(x, lr01_hist_train_loss, label='lr=0.01')\n",
    "lr001_line, = plt.plot(x, lr001_hist_train_loss, label='lr=0.001')\n",
    "plt.ylim([1,2])\n",
    "plt.xlabel('#epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.subplot(122)\n",
    "x = [x for x in range(len(lr1_hist_train_acc))]\n",
    "lr1_line, = plt.plot(x, lr1_hist_train_acc, label='lr=0.1')\n",
    "lr01_line, = plt.plot(x, lr01_hist_train_acc, label='lr=0.01')\n",
    "lr001_line, = plt.plot(x, lr001_hist_train_acc, label='lr=0.001')\n",
    "plt.xlabel('#epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend(handles=[lr1_line,lr01_line,lr001_line], loc=4)\n",
    "plt.savefig('./training_curve/ex02_LearningRate.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
