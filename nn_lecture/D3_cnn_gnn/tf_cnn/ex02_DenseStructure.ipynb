{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import [package name] as [name abbr.]\n",
    "# python 處理數字跟 tensor 運算的主要套件\n",
    "import numpy as np\n",
    "import math\n",
    "# google 的 NN coding 套件\n",
    "import tensorflow as tf\n",
    "\n",
    "# 監控進程的套件\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data frame 的好用套件\n",
    "import pandas\n",
    "\n",
    "# 有可能會出現警告，但是可以不用理他\n",
    "# c:\\users\\silver\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:36:\n",
    "# FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating`\n",
    "# is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
    "# from ._conv import register_converters as _register_converters\n",
    "import matplotlib.pyplot as plt\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dataset path\n",
    "dataset_path = './cifar_10/'\n",
    "height, width, dim = 32, 32, 3\n",
    "classes = 10\n",
    "X_train, X_test, Y_train, Y_test = utils.read_dataset(dataset_path, \"img\") "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ex 02\n",
    "建立一個 Convolutional Neural Network with dense structure (input_dim = [32,32,3], output_dim = 10)\n",
    "\n",
    "1st CNN layer: 32 個 [3,3] filters with 'ReLU' activation function and batch normalization layer sequentially\n",
    "2nd CNN layer: 32 個 [3,3] filters with 'ReLU' activation function and batch normalization layer sequentially\n",
    "1st concate layer: 將 1st 與 2nd 的 output 黏接起來\n",
    "3rd CNN layer: 32 個 [3,3] filters with 'ReLU' activation function and batch normalization layer sequentially\n",
    "2nd concate layer: 將 1st, 2nd, and 3rd 的 output 黏接起來\n",
    "4th CNN layer: 32 個 [3,3] filters with 'ReLU' activation function and batch normalization layer and maxpooling layer\n",
    "\n",
    "Flatten Layer\n",
    "1st FC layer: 512 nodes with with 'ReLU' activation function and batch normalization layer sequentially\n",
    "2nd FC layer (output): 10 noeds\n",
    "\n",
    "Hint:\n",
    "tf.layers.conv2d(prev_layer, filters, kernel_size, strides, padding)\n",
    "tf.layers.max_pooling2d(prev_layer, pool_size, strides, padding)\n",
    "tf.layers.flatten(prev_layer)\n",
    "tf.concat([layer1, layer2,...], axis = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model Input (x) and Output (y_),  y_ = f(x)\n",
    "x = tf.placeholder(tf.float32, [None, height, width, dim])\n",
    "y_ = tf.placeholder(tf.int32, [None])\n",
    "phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
    "y_one = tf.one_hot(y_,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_cnn1 = tf.layers.conv2d(inputs=x,\n",
    "                           filters=32,\n",
    "                           kernel_size=[3, 3],\n",
    "                           strides=1,\n",
    "                           padding=\"same\")\n",
    "ds_cnn1_act = tf.nn.relu(ds_cnn1)\n",
    "ds_cnn1_bn = tf.layers.batch_normalization(ds_cnn1_act,training=phase_train)\n",
    "\n",
    "ds_cnn2 = tf.layers.conv2d(inputs=ds_cnn1_bn,\n",
    "                           filters=32,\n",
    "                           kernel_size=[3, 3],\n",
    "                           strides=1,\n",
    "                           padding=\"same\")\n",
    "ds_cnn2_act = tf.nn.relu(ds_cnn2)\n",
    "ds_cnn2_bn = tf.layers.batch_normalization(ds_cnn2_act,training=phase_train)\n",
    "\n",
    "ds_concate1 = \n",
    "\n",
    "ds_cnn3 = tf.layers.conv2d(inputs=ds_cnn2_bn,\n",
    "                           filters=32,\n",
    "                           kernel_size=[3, 3],\n",
    "                           strides=1,\n",
    "                           padding=\"same\")\n",
    "ds_cnn3_act = tf.nn.relu(ds_cnn3)\n",
    "ds_cnn3_bn = tf.layers.batch_normalization(ds_cnn3_act,training=phase_train)\n",
    "\n",
    "ds_concate2 = \n",
    "\n",
    "ds_cnn4 = tf.layers.conv2d(inputs=ds_concate2,\n",
    "                           filters=32,\n",
    "                           kernel_size=[3, 3],\n",
    "                           strides=1,\n",
    "                           padding=\"same\")\n",
    "ds_cnn4_act = tf.nn.relu(ds_cnn4)\n",
    "ds_cnn4_bn = tf.layers.batch_normalization(ds_cnn4_act,training=phase_train)\n",
    "\n",
    "ds_maxpool1 = tf.layers.max_pooling2d(ds_cnn4_bn,\n",
    "                                      pool_size=[2,2],\n",
    "                                      strides=2,\n",
    "                                      padding='same')\n",
    "\n",
    "features = tf.layers.flatten(ds_maxpool1)\n",
    "\n",
    "ds_h1 = tf.layers.dense(features,512,activation=None)\n",
    "ds_h1_act = tf.nn.relu(ds_h1)\n",
    "ds_h1_bn = tf.layers.batch_normalization(ds_h1_act,training=phase_train)\n",
    "ds_y = tf.layers.dense(ds_h1_bn,classes,activation=None)\n",
    "\n",
    "ds_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_one, logits=ds_y))\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "ds_train_step = tf.train.AdamOptimizer(0.001).minimize(ds_loss)\n",
    "\n",
    "# Accuracy of the Model\n",
    "ds_y_pred = tf.argmax(tf.nn.softmax(ds_y), 1, output_type=tf.int32)\n",
    "ds_correct = tf.equal(ds_y_pred, y_)\n",
    "ds_accuracy = tf.reduce_mean(tf.cast(ds_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the models\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 2\n",
    "batch_size = 128\n",
    "get_train_batches = utils.gen_batches(X_train, Y_train, batch_size)\n",
    "get_valid_batches = utils.gen_batches(X_test, Y_test, batch_size)\n",
    "batches_in_a_epoch = int(X_train.shape[0]/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_hist_train_acc = []\n",
    "ds_hist_valid_acc = []\n",
    "ds_hist_train_loss = []\n",
    "ds_hist_valid_loss = []\n",
    "\n",
    "for step in tqdm(range(epoch*batches_in_a_epoch)):\n",
    "    batch_train_xs, batch_train_ys = next(get_train_batches)\n",
    "    batch_valid_xs, batch_valid_ys = next(get_valid_batches)\n",
    "    sess.run([ds_train_step,update_ops], feed_dict={x: batch_train_xs, y_: batch_train_ys, phase_train:True})\n",
    "    if (step % batches_in_a_epoch == 0):\n",
    "        # trainin and validation evaluation\n",
    "        train_loss, train_acc = sess.run([ds_loss,ds_accuracy], feed_dict={x: batch_train_xs, y_:batch_train_ys, phase_train:False})\n",
    "        valid_loss, valid_acc = sess.run([ds_loss,ds_accuracy], feed_dict={x: batch_valid_xs,  y_: batch_valid_ys, phase_train:False})\n",
    "        ds_hist_train_acc.append(train_acc)\n",
    "        ds_hist_valid_acc.append(valid_acc)\n",
    "        ds_hist_train_loss.append(train_loss)\n",
    "        ds_hist_valid_loss.append(valid_loss)\n",
    "        #print(\"Accuracy: [T] %.4f / [V] %.4f\" % (train_acc,valid_acc))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0,figsize=(8,6))\n",
    "plt.subplot(121)\n",
    "x = [x for x in range(len(ds_hist_valid_loss))]\n",
    "line_train, = plt.plot(x, ds_hist_train_loss, label='Training')\n",
    "line_valid, = plt.plot(x, ds_hist_valid_loss, label='Validation')\n",
    "plt.xlabel('#epoch')\n",
    "plt.ylabel('Loss')\n",
    "#plt.ylim([0,2])\n",
    "plt.legend(handles=[line_train,line_valid], loc=4)\n",
    "plt.subplot(122)\n",
    "x = [x for x in range(len(ds_hist_valid_acc))]\n",
    "line_train, = plt.plot(x, ds_hist_train_acc, label='Training')\n",
    "line_valid, = plt.plot(x, ds_hist_valid_acc, label='Validation')\n",
    "plt.xlabel('#epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0,1])\n",
    "plt.legend(handles=[line_train,line_valid], loc=4)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./training_curve/ex02_DenseStructure.png',dpi=300,format='png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存入.csv檔\n",
    "res = pandas.DataFrame({\"val_acc\":ds_hist_valid_acc,\n",
    "                        \"val_loss\":ds_hist_valid_loss,\n",
    "                        \"acc\":ds_hist_train_acc,\n",
    "                        \"loss\":ds_hist_train_loss})\n",
    "res.to_csv('./training_curve/ex02_DenseStructure.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
