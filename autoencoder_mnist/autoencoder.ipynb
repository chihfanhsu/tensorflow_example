{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mmnet\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(MAX_ITERATION=100000.0, batch_size=100, gpu=1, lr=0.0001, tar_model='model_1', training=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pickle\n",
    "import tqdm\n",
    "import config\n",
    "conf, _ = config.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Setting visable GPU'''\n",
    "gpus = [conf.gpu] # Here I set CUDA to only see one GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=','.join([str(i) for i in gpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './'+conf.tar_model+'/'\n",
    "data_dir = './dataset/'\n",
    "logs_dir = model_dir + 'logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model_1\n"
     ]
    }
   ],
   "source": [
    "if conf.tar_model == 'model_1':\n",
    "    import model_1 as model\n",
    "    print('Load', conf.tar_model)\n",
    "else:\n",
    "    sys.exit(\"Sorry, Wrong Model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-9c608aa028e2>:6: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data.\n",
      "WARNING:tensorflow:From c:\\users\\mmnet\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\__init__.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\users\\mmnet\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\users\\mmnet\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From c:\\users\\mmnet\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\mmnet\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\mmnet\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# from sparse matrix\n",
    "# temp = mmread('./sparse_behavior_data.csv')\n",
    "# den_data = temp.toarray().astype(np.uint16)\n",
    "\n",
    "# from dense array\n",
    "mnist = tf.contrib.learn.datasets.load_dataset('mnist')\n",
    "\n",
    "raw_data = mnist.train.images # Returns np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(55000, 784)\n"
     ]
    }
   ],
   "source": [
    "# check shapes of raw_data\n",
    "print(type(raw_data))\n",
    "print(raw_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing for the data\n",
    "processed_data = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(55000, 784)\n"
     ]
    }
   ],
   "source": [
    "# check shapes of processed_data\n",
    "print(type(processed_data))\n",
    "print(processed_data.shape)\n",
    "#den_data=den_data.astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting a batch generator for getting a batch of dataset\n",
    "def batch_generator(data, idx, batch_size = 128):\n",
    "    # while (1):\n",
    "    while(True):\n",
    "        np.random.shuffle(idx)\n",
    "        for i in range(int(len(idx)/batch_size)):\n",
    "            yield data[idx[i*batch_size:((i+1)*batch_size)]]\n",
    "            # you might have a training label, and the output will be \n",
    "            # yield traing_data[idx[i*batch_size:((i+1)*batch_size)]], yield traing_label[idx[i*batch_size:((i+1)*batch_size)]]\n",
    "            \n",
    "# split training and validation set\n",
    "def get_generators(training_data, tv_ratio = 0.8, shuffle = True, batch_size = 128):\n",
    "    \n",
    "    print('Size of all training data', training_data.shape)\n",
    "    idx = np.arange(0, training_data.shape[0], 1)\n",
    "    if shuffle:\n",
    "        print('*Shuffle before training and validation split')\n",
    "        np.random.shuffle(idx)\n",
    "    training_idx = idx[:(int(tv_ratio*len(idx)))]\n",
    "    validation_idx =idx[int(tv_ratio*len(idx)):]\n",
    "    if (len(training_idx) + len(validation_idx) - len(idx)) > 0:\n",
    "        print('Some data are duplicated in the training and validation split')\n",
    "    elif (len(training_idx) + len(validation_idx) - len(idx)) < 0:\n",
    "        print('Some data are missing in the training and validation split')\n",
    "\n",
    "    training_gen = batch_generator(training_data, training_idx, batch_size)\n",
    "    validation_gen = batch_generator(training_data, validation_idx, batch_size)\n",
    "    return training_gen, len(training_idx), validation_gen, len(validation_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of all training data (55000, 784)\n",
      "*Shuffle before training and validation split\n"
     ]
    }
   ],
   "source": [
    "# defining the batch generators\n",
    "if conf.training:\n",
    "    get_training_batch, n_training_samples, get_validation_batch, n_validation_samples = get_generators(processed_data, tv_ratio = 0.8, shuffle = True, batch_size = conf.batch_size) # (128, 30, 450)\n",
    "else:\n",
    "    testing_idx = np.arange(0, processed_data.shape[0], 1)\n",
    "    get_testing_batch = batch_generator(processed_data, testing_idx, conf.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = next(get_training_batch)\n",
    "# print(b.shape)\n",
    "# print(n_training_samples)\n",
    "\n",
    "# b = next(get_validation_batch)\n",
    "# print(b.shape)\n",
    "# print(n_validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_parameters 942656\n",
      "Setting up summary op...\n",
      "Setting up Saver...\n",
      "-------- Start Training --------\n",
      "[T] Step: 0, loss:0.113827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 110/110 [00:00<00:00, 746.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[V*] Step: 0, loss:0.113011\n",
      "[T] Step: 2000, loss:0.0359481\n",
      "[T] Step: 4000, loss:0.028422\n",
      "[T] Step: 6000, loss:0.0243834\n",
      "[T] Step: 8000, loss:0.0232882\n",
      "[T] Step: 10000, loss:0.0196861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 110/110 [00:00<00:00, 729.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[V*] Step: 10000, loss:0.0209793\n",
      "[T] Step: 12000, loss:0.0186628\n",
      "[T] Step: 14000, loss:0.0187184\n",
      "[T] Step: 16000, loss:0.0177832\n",
      "[T] Step: 18000, loss:0.0176104\n",
      "[T] Step: 20000, loss:0.0170484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 110/110 [00:00<00:00, 740.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[V*] Step: 20000, loss:0.0174565\n",
      "[T] Step: 22000, loss:0.0162234\n",
      "[T] Step: 24000, loss:0.0167638\n",
      "[T] Step: 26000, loss:0.0159327\n",
      "[T] Step: 28000, loss:0.016162\n",
      "[T] Step: 30000, loss:0.0148709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 110/110 [00:00<00:00, 731.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[V*] Step: 30000, loss:0.0158154\n",
      "[T] Step: 32000, loss:0.0148745\n",
      "[T] Step: 34000, loss:0.0166909\n",
      "[T] Step: 36000, loss:0.0147977\n",
      "[T] Step: 38000, loss:0.0133016\n",
      "[T] Step: 40000, loss:0.0146284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 110/110 [00:00<00:00, 717.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[V*] Step: 40000, loss:0.01496\n",
      "[T] Step: 42000, loss:0.014039\n",
      "[T] Step: 44000, loss:0.01391\n",
      "[T] Step: 46000, loss:0.0129806\n",
      "[T] Step: 48000, loss:0.0140725\n",
      "[T] Step: 50000, loss:0.0127213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 110/110 [00:00<00:00, 728.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[V*] Step: 50000, loss:0.0141388\n",
      "[T] Step: 52000, loss:0.0129673\n",
      "[T] Step: 54000, loss:0.0123059\n",
      "[T] Step: 56000, loss:0.0140105\n",
      "[T] Step: 58000, loss:0.0132247\n",
      "[T] Step: 60000, loss:0.0119239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 110/110 [00:00<00:00, 726.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[V*] Step: 60000, loss:0.0136684\n",
      "[T] Step: 62000, loss:0.0124587\n",
      "[T] Step: 64000, loss:0.0124789\n",
      "[T] Step: 66000, loss:0.0118648\n",
      "[T] Step: 68000, loss:0.0131211\n",
      "[T] Step: 70000, loss:0.0115306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 110/110 [00:00<00:00, 725.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[V*] Step: 70000, loss:0.013314\n",
      "[T] Step: 72000, loss:0.0132774\n",
      "[T] Step: 74000, loss:0.0123373\n",
      "[T] Step: 76000, loss:0.0127052\n",
      "[T] Step: 78000, loss:0.0125724\n"
     ]
    }
   ],
   "source": [
    "# clear tensorflow graph\n",
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default() as g:\n",
    "    \n",
    "    ###############\n",
    "    # Model Setup #\n",
    "    ###############\n",
    "    \n",
    "    # model input\n",
    "    model_input = tf.placeholder(tf.float32, shape=[conf.batch_size, 784], name=\"model_input\")\n",
    "    model_GT = tf.placeholder(tf.float32, shape=(conf.batch_size, 784), name='model_GT')\n",
    "    keep_probability = tf.placeholder(tf.float32, name=\"keep_probabilty\")\n",
    "    train_phase = tf.placeholder(tf.bool, name='phase_train')\n",
    "\n",
    "    # build model\n",
    "    code, model_out = model.inference(model_input, keep_probability, train_phase)\n",
    "\n",
    "    # check # parameter of the model\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        # shape is an array of tf.Dimension\n",
    "        shape = variable.get_shape()\n",
    "        #   print(shape)\n",
    "        #   print(len(shape))\n",
    "        variable_parameters = 1\n",
    "        for dim in shape:\n",
    "        #   print(dim)\n",
    "            variable_parameters *= dim.value\n",
    "        #   print(variable_parameters)\n",
    "        total_parameters += variable_parameters\n",
    "    print('total_parameters', total_parameters)\n",
    "    \n",
    "    # model loss\n",
    "    loss = tf.reduce_mean(tf.losses.mean_squared_error(predictions=model_out, labels=model_GT))\n",
    "    loss_summary = tf.summary.scalar(\"Tot_loss\", loss)\n",
    "\n",
    "    def train(loss_val, var_list):\n",
    "        optimizer = tf.train.AdamOptimizer(conf.lr)\n",
    "        grads = optimizer.compute_gradients(loss_val, var_list=var_list)\n",
    "        return optimizer.apply_gradients(grads)\n",
    "\n",
    "    trainable_var = tf.trainable_variables()\n",
    "    \n",
    "    train_op = train(loss, trainable_var)\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    # setup\n",
    "    print(\"Setting up summary op...\")\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "    print(\"Setting up Saver...\")\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False), graph=g)\n",
    "    \n",
    "    if (conf.training == False):\n",
    "        ckpt = tf.train.get_checkpoint_state(logs_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            print('Loading sucessfully')\n",
    "        else:\n",
    "            print('No checkpoint file found')\n",
    "            raise\n",
    "    else:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "    # create two summary writers to show training loss and validation loss in the same graph\n",
    "    # need to create two folders 'train' and 'validation' inside FLAGS.logs_dir\n",
    "    train_writer = tf.summary.FileWriter(logs_dir+'/train', sess.graph)\n",
    "    validation_writer = tf.summary.FileWriter(logs_dir+'/validation', sess.graph)\n",
    "    \n",
    "    #########\n",
    "    # Start #\n",
    "    #########\n",
    "    if conf.training == True:\n",
    "        print(\"-------- Start Training --------\")\n",
    "        max_validloss = 99999\n",
    "        for itr in range(int(conf.MAX_ITERATION)):\n",
    "            # prepare training input\n",
    "            batch_xs = next(get_training_batch)\n",
    "\n",
    "            sess.run([train_op,extra_update_ops], feed_dict={model_input: batch_xs,\n",
    "                                                             model_GT: batch_xs,\n",
    "                                                             keep_probability: 0.85,\n",
    "                                                             train_phase:conf.training})\n",
    "\n",
    "            if itr % 2000 == 0:\n",
    "                train_loss, summary_str = sess.run([loss, loss_summary], feed_dict={model_input: batch_xs,\n",
    "                                                                                     model_GT: batch_xs,\n",
    "                                                                                     keep_probability: 1,\n",
    "                                                                                     train_phase:False})\n",
    "                print(\"[T] Step: %d, loss:%g\" % (itr, np.mean(train_loss)))\n",
    "                train_writer.add_summary(summary_str, itr)\n",
    "                \n",
    "            # validation\n",
    "            if itr % 10000 == 0:\n",
    "                # prepare inputs\n",
    "                valid_losses = []\n",
    "                for i in tqdm.trange(int(n_validation_samples/conf.batch_size)):\n",
    "                    batch_xs_valid = next(get_validation_batch)\n",
    "                    \n",
    "                    valid_loss, summary_sva=sess.run([loss, loss_summary], feed_dict={model_input: batch_xs_valid,\n",
    "                                                                                     model_GT: batch_xs_valid,\n",
    "                                                                                     keep_probability: 1,\n",
    "                                                                                     train_phase:False})\n",
    "                    valid_losses.append(valid_loss)\n",
    "                    \n",
    "                # save validation log\n",
    "                validation_writer.add_summary(summary_sva, itr)\n",
    "                # save the ckpt if reachings better loss\n",
    "                calc_v_loss = np.mean(valid_losses)\n",
    "\n",
    "                if calc_v_loss < max_validloss:\n",
    "                    saver.save(sess, logs_dir + \"model.ckpt\", itr)\n",
    "                    print(\"[V*] Step: %d, loss:%g\" % (itr, calc_v_loss))\n",
    "                    max_validloss = calc_v_loss\n",
    "                else:\n",
    "                    print(\"[V] Step: %d, loss:%g\" % (itr, calc_v_loss))\n",
    "    else:\n",
    "        print(\"Start Testing\")\n",
    "\n",
    "        test_losses = []\n",
    "        codes=[]\n",
    "        for i in tqdm.trange(int(processed_data.shape[0]/conf.batch_size)):\n",
    "            batch_xs_test = next(get_testing_batch)\n",
    "\n",
    "            coder, test_loss=sess.run([code, loss], feed_dict={model_input: batch_xs_test,\n",
    "                                                                             model_GT: batch_xs_test,\n",
    "                                                                             keep_probability: 1,\n",
    "                                                                             train_phase:False})\n",
    "            test_losses.append(test_loss)\n",
    "            codes.append(coder)\n",
    "\n",
    "\n",
    "        # save the ckpt if reachings better loss\n",
    "        calc_test_loss = np.mean(test_losses)\n",
    "        print(\"[Test] Avg. loss:%g\" % (calc_test_loss))\n",
    "        a = np.array(codes); print(a.shape)\n",
    "        b = np.swapaxes(a, 0, 1); print(b.shape)\n",
    "        c = b.reshape(-1, b.shape[2]);print(c.shape)\n",
    "        import pandas\n",
    "        encode = pandas.DataFrame(c)\n",
    "        encode.to_csv('encode.csv',index = False)\n",
    "        \n",
    "    sess.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
